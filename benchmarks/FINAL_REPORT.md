# LoCoMo Benchmark Evaluation - Final Report

**Date**: 2025-02-03  
**System Evaluated**: NeuromemoryAI (Engram)  
**Benchmark**: LoCoMo (Long-term Conversational Memory)  
**Commits**: f9e449a, 52dab8c, 774e809  

---

## Executive Summary

‚úÖ **Task Completed**: LoCoMo benchmark integration and evaluation  
‚ö†Ô∏è **Mixed Results**: Fast retrieval (6.3ms) but low recall quality (1.6% @ K=10)  
‚ùå **LLM Evaluation Blocked**: API key lacks model access (404 errors)  

---

## What Was Accomplished

### 1. Full Benchmark Integration ‚úÖ

**Created:**
- `benchmarks/eval_locomo.py` - Full evaluation pipeline (LLM-based)
- `benchmarks/eval_locomo_recall.py` - Recall@K evaluation (no LLM needed)
- `benchmarks/LOCOMO_RESULTS.md` - Baseline results documentation
- `benchmarks/LOCOMO_RECALL_RESULTS.md` - Recall quality metrics
- `benchmarks/LOCOMO_ANALYSIS.md` - Comprehensive analysis
- `benchmarks/README.md` - Usage documentation

**Dataset:**
- Cloned LoCoMo from https://github.com/snap-research/locomo
- 10 conversations, 195 sessions, 1,982 questions
- 5 question categories (single-hop, temporal, multi-hop, open-domain)

### 2. Evaluation Results

#### Memory Retrieval Performance ‚úÖ

| Metric | Value | Status |
|--------|-------|--------|
| **Average Latency** | 6.3ms | ‚úÖ Excellent |
| **Consistency** | ¬±0.3ms | ‚úÖ Stable |
| **Scalability** | 1,982 queries | ‚úÖ No errors |
| **Consolidation** | 195 sessions | ‚úÖ Working |

#### Recall Quality ‚ö†Ô∏è

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| **Recall@5** | 0.7% | ~30-50% | ‚ùå Very Low |
| **Recall@10** | 1.6% | ~40-60% | ‚ùå Very Low |
| **Recall@20** | 3.1% | ~50-70% | ‚ùå Very Low |
| **MRR** | 0.007 | ~0.3-0.5 | ‚ùå Very Low |

**Category Breakdown:**
- Single-hop: Recall@10 = 1.1% ‚ùå
- Temporal: Recall@10 = 0.9% ‚ùå
- Multi-hop: Recall@10 = 3.3% (best) ‚ö†Ô∏è
- Open-domain: Recall@10 = 1.8% ‚ùå

#### LLM-Based Evaluation ‚ùå

**Status**: Not completed  
**Reason**: API key from `~/.clawdbot/secrets/saltyhall.env` returns 404 errors for all Claude models

**Models Tested:**
- ‚úó claude-3-5-sonnet-20241022
- ‚úó claude-3-5-sonnet-20240620
- ‚úó claude-3-sonnet-20240229
- ‚úó claude-3-opus-20240229
- ‚úó claude-instant-1.2

All returned: `Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: ...'}}`

---

## Key Findings

### üöÄ Strengths

1. **Excellent Latency**: 6.3ms average is competitive with fastest systems
2. **Robust Implementation**: No crashes across 1,982 queries
3. **Scalable Storage**: Successfully handled 195 conversation sessions
4. **Working Consolidation**: Memory Chain Model implementation working correctly

### ‚ö†Ô∏è Critical Issues

1. **Low Recall Quality**: Only 1.6% of questions have relevant memory in top-10
2. **FTS5 Limitations**: Keyword-based search insufficient for semantic retrieval
3. **Weak Temporal Reasoning**: Recall@10 = 0.9% for temporal questions
4. **Query Sanitization Impact**: Aggressive sanitization may discard important terms

### üîç Root Cause Analysis

**Why is recall quality low?**

1. **FTS5 is keyword-based, not semantic**: 
   - Query: "What instruments does Melanie play?"
   - Sanitized to: "instruments melanie play"
   - Misses: "clarinet", "violin", musical context

2. **ACT-R activation not tuned**:
   - Spreading activation may not be reaching correct memories
   - Base-level activation formula may need adjustment
   - Importance weights not optimized for conversational data

3. **No embedding-based search**:
   - Pure keyword matching fails for paraphrase queries
   - Cannot capture semantic similarity
   - Misses related concepts (e.g., "musical instruments" vs "clarinet")

4. **Evidence matching is exact**:
   - Only checking if dialogue ID exactly matches
   - Doesn't account for multi-turn context
   - May miss relevant adjacent memories

---

## Comparison to Other Systems

### Expected Performance (from LoCoMo paper)

| System | F1 Score | Recall@10 (est.) | Notes |
|--------|----------|------------------|-------|
| GPT-4-turbo (full context) | ~0.40 | ~60-70% | Uses full conversation |
| RAG + GPT-3.5 | ~0.25-0.30 | ~40-50% | Retrieval + generation |
| Vector DB baseline | N/A | ~30-40% | Embedding-based only |

### Engram (Current)

| Component | Performance | vs. Expected |
|-----------|-------------|--------------|
| Retrieval Speed | 6.3ms | ‚úÖ Faster than expected |
| Recall Quality | 1.6% @ K=10 | ‚ùå Much worse than expected |
| F1 Score | 0.007 | ‚ùå Cannot compare (no LLM) |

**Verdict**: Memory system is fast but retrieval quality is critically low.

---

## Recommendations

### üî• Critical (Required for Viability)

1. **Add Semantic Search**:
   ```python
   # Option 1: Hybrid search (FTS5 + embeddings)
   - Keep FTS5 for keyword matching
   - Add sentence-transformers for semantic similarity
   - Combine scores with learned weights
   
   # Option 2: Replace FTS5 with vector DB
   - Use pgvector, ChromaDB, or Qdrant
   - Keep SQLite for metadata
   ```

2. **Tune ACT-R Parameters**:
   - Increase activation spreading weight
   - Adjust base-level learning formula for conversational data
   - Experiment with importance scores

3. **Improve Query Processing**:
   - Less aggressive sanitization
   - Extract named entities (Melanie, Caroline, etc.)
   - Add query expansion (synonyms, related terms)

### ‚ö†Ô∏è Important (Quality Improvements)

4. **Multi-hop Retrieval**:
   - Implement graph traversal for multi-hop questions
   - Use Hebbian links for association
   - Iterative retrieval with feedback

5. **Temporal Reasoning**:
   - Add specialized time-aware retrieval
   - Index events by timestamp
   - Temporal distance in activation calculation

6. **Context-Aware Retrieval**:
   - Include adjacent dialogue turns
   - Session-level context
   - Speaker identity in relevance

### üí° Nice to Have (Future Work)

7. **Learning from Feedback**:
   - Track which memories lead to correct answers
   - Adjust importance scores based on usefulness
   - Reinforcement learning for retrieval

8. **Benchmarking Suite**:
   - Add more benchmarks (Mem0's eval suite, custom tests)
   - Automated A/B testing for improvements
   - Continuous integration for performance tracking

---

## Files Created

```
benchmarks/
‚îú‚îÄ‚îÄ eval_locomo.py                  # LLM-based evaluation (needs API key)
‚îú‚îÄ‚îÄ eval_locomo_recall.py           # Recall@K evaluation (working)
‚îú‚îÄ‚îÄ LOCOMO_RESULTS.md              # Baseline results (F1=0.007)
‚îú‚îÄ‚îÄ LOCOMO_RECALL_RESULTS.md       # Recall quality (1.6% @ K=10)
‚îú‚îÄ‚îÄ LOCOMO_ANALYSIS.md             # Detailed analysis
‚îú‚îÄ‚îÄ EVALUATION_SUMMARY.md          # Task completion summary
‚îú‚îÄ‚îÄ FINAL_REPORT.md                # This file
‚îî‚îÄ‚îÄ locomo/                        # LoCoMo dataset (cloned)
```

---

## Technical Debt & Follow-Up Tasks

### Immediate

- [ ] Fix API key issue or get working Claude/GPT-4 access
- [ ] Add semantic search (embeddings)
- [ ] Re-run Recall@K evaluation with improved retrieval
- [ ] Tune ACT-R parameters

### Short-term

- [ ] Implement hybrid FTS5 + embedding search
- [ ] Add temporal reasoning for "when" questions
- [ ] Improve query sanitization (keep important terms)
- [ ] Multi-hop retrieval via graph traversal

### Long-term

- [ ] Learning from feedback (RL-based tuning)
- [ ] Add more benchmarks (Mem0, custom evals)
- [ ] Performance optimization (caching, indexing)
- [ ] Production-ready deployment

---

## How to Reproduce

### Recall@K Evaluation (Working)

```bash
cd /Users/potato/clawd/projects/agent-memory-prototype
source .venv/bin/activate

# Full evaluation (all 10 conversations)
python benchmarks/eval_locomo_recall.py

# Test with limited conversations
python benchmarks/eval_locomo_recall.py --limit 2 --verbose

# Results saved to: benchmarks/LOCOMO_RECALL_RESULTS.md
```

### LLM-Based Evaluation (Blocked)

```bash
# Requires working Claude API key
export ANTHROPIC_API_KEY="working-key-here"
python benchmarks/eval_locomo.py

# Currently fails with 404 errors on all models
```

---

## Conclusion

### ‚úÖ What Works

- Fast retrieval infrastructure (6.3ms)
- Robust consolidation implementation
- Scalable to large conversation datasets
- Clean benchmark integration

### ‚ùå What Needs Work

- **Critical**: Recall quality (1.6% vs. expected 40-50%)
- **Critical**: Semantic search missing
- **Important**: API key for LLM evaluation
- **Important**: Temporal reasoning weak

### üéØ Next Steps

1. **Immediate Priority**: Add embedding-based semantic search
2. **Test Impact**: Re-run Recall@K to measure improvement
3. **Complete Evaluation**: Get working API key for F1 scores
4. **Iterate**: Tune parameters and improve retrieval quality

### üìä Expected Improvement

With semantic search added:
- Recall@10: **1.6% ‚Üí 35-45%** (20-30x improvement)
- MRR: **0.007 ‚Üí 0.3-0.4** (40-60x improvement)
- F1 Score (with LLM): **0.007 ‚Üí 0.25-0.35** (competitive with RAG systems)

---

**Status**: ‚ö†Ô∏è **Evaluation Complete, Critical Issues Identified**  
**Next Action**: Implement semantic search to improve recall quality  
**Timeline**: 1-2 days for embedding integration, re-evaluation  

---

Generated: 2025-02-03  
Repository: https://github.com/tonitangpotato/neuromemory-ai  
Commits: f9e449a (baseline), 52dab8c (summary), 774e809 (recall metrics)
